<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Sindhuja Chaduvula â€” AI Safety | Governance | Explainability</title>
  <meta name="description" content="Sindhuja Chaduvula â€” Associate Applied ML Specialist at Vector Institute. Research in AI Safety, Explainability, and Governance-aligned LLMs." />
  <meta name="keywords" content="Sindhuja Chaduvula, AI Safety, AI Governance, Explainability, Vector Institute, Responsible AI, Generative AI, AAAI Reviewer" />

  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&family=Playfair+Display:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" />

  <style>
    body{font-family:'Roboto',sans-serif;line-height:1.7;color:#333;background:#fff;}
    h1,h2,h3,h4{font-family:'Playfair Display',serif;color:#0d1b2a;}
    header{text-align:center;padding:2rem 0;background:linear-gradient(to right,#f8fafc,#e6eef3);border-bottom:1px solid #dee2e6;}
    header img{border-radius:50%;width:150px;height:150px;object-fit:cover;margin-bottom:1rem;}
    header p{margin-bottom:.4rem;color:#6c757d;}
    a{color:#007bff;text-decoration:none;}a:hover{text-decoration:underline;}
    .section-title{margin-top:2.5rem;font-size:1.75rem;border-bottom:3px solid #dee2e6;padding-bottom:.5rem;}
    .highlight-box{background:#f1f3f5;border-left:4px solid #007bff;padding:1rem;margin-bottom:2rem;border-radius:6px;}
    .award-banner{background:#fff8e1;border:1px solid #ffe082;border-radius:8px;padding:1.5rem;margin-top:1.5rem;}
    .award-banner img{max-height:200px;border-radius:8px;}
    footer{text-align:center;padding:1.5rem 0;background:#e9ecef;margin-top:3rem;font-size:.9rem;}
    ul{padding-left:1.3rem;}ul li{margin-bottom:.6rem;}
    details summary{cursor:pointer;font-weight:600;}
    details summary::-webkit-details-marker{display:none;}
    details summary::after{content:" â–¶";}details[open] summary::after{content:" â–¼";}
  </style>
</head>

<body class="container">

<header>
  <img src="Photo.JPG" alt="Sindhuja Chaduvula" />
  <h1>Sindhuja Chaduvula</h1>
  <p><strong>Associate Applied ML Specialist Â· AI Safety Â· Governance Â· Explainability</strong></p>
  <p>Vector Institute | University of Waterloo</p>
  <p><a href="mailto:sindhu.chaduvula.21@gmail.com">sindhu.chaduvula.21@gmail.com</a> | <a href="https://linkedin.com/in/sindhujachaduvula">LinkedIn</a> | <a href="https://github.com/sindhujachaduvula">GitHub</a></p>
</header>

<section>
  <h2 class="section-title">About Me</h2>
  <div class="highlight-box">
    <p>Iâ€™m an <strong>Applied ML Specialist at the Vector Institute</strong>, working on <strong>AI Safety, Explainability, and Governance-aligned Generative AI systems</strong>. My research bridges applied machine learning and responsible deployment to build trustworthy LLMs and multi-agent architectures.</p>
    <p>I co-authored <em>â€œExplainability and Interpretability in Agentic AI: A Survey Across the Agentic Loop (2025)â€</em>, leading the Explainability section that mapped intrinsic, post-hoc, causal, and prototype-based methods for LLM-driven agents. This work ties interpretability to global frameworks like the EU AI Act, NIST AI RMF, and ISO/IEC 42001.</p>
    <p>I also served as a <strong>Reviewer for the AAAI 2026 Workshop on AI Governance (AIGOV)</strong>, reviewing papers on LLM Governance and AI Safety focusing on human oversight and policy-aligned accountability.</p>
  </div>
</section>

<section>
  <h2 class="section-title">Research Focus</h2>
  <ul>
    <li>AI Safety and Governance-aware Explainability Frameworks</li>
    <li>Preference Alignment (DPO / ORPO / KTO)</li>
    <li>Trust-weighted Interpretability for Alignment Auditing</li>
    <li>Retrieval-Augmented Generation (RAG) and Multi-Agent Reasoning</li>
    <li>Ethical and Human-in-the-Loop AI Deployment</li>
  </ul>
</section>

<section>
  <h2 class="section-title">Experience</h2>

  <h4>ğŸ§© Vector Institute â€” Associate Applied ML Specialist (2025 â€“ Present)</h4>
  <ul>
    <li>Conducting research on <strong>AI Safety</strong> and <strong>Governance-Aware Explainability</strong> for LLMs.</li>
    <li>Generated 250+ synthetic scenarios using GPT-4o Mini for bias and fairness audits.</li>
    <li>Designed trust-weighted interpretability mechanisms for alignment auditing and safety verification.</li>
  </ul>

  <h4>ğŸ’¡ Vector Institute â€” Machine Learning Associate (2025)</h4>
  <ul>
    <li>Built a LangChain + Qdrant RAG pipeline for 1,000-page medical billing documents (âˆ’80% lookup time).</li>
    <li>Implemented hybrid retrieval (OpenAI Embeddings + BM25) with safety filters and governance constraints.</li>
  </ul>

  <h4>ğŸ§® Genellipse â€” Data Scientist (2024)</h4>
  <ul>
    <li>Fine-tuned Gemma 2B, LLaMA 3, and Mistral 7B on SEC 10-Q filings to automate financial narratives.</li>
    <li>Automated data pipelines via EDGAR API and AWS S3, ensuring transparency and traceability.</li>
  </ul>

  <h4>ğŸ¤– AIFocal Inc. â€” AI Engineer Intern (2023)</h4>
  <ul>
    <li>Built a PDF-aware QA chatbot using Gemma 2B and LangChain RAG with Milvus retrieval.</li>
    <li>Designed semantic prompt-routing reducing hallucinations by 12% in long-context tasks.</li>
  </ul>

  <h4>ğŸ“Š Tata Consultancy Services â€” Assistant System Engineer (2021 â€“ 2022)</h4>
  <ul>
    <li>Developed telecom churn prediction models (Random Forest, XGBoost) with 87% accuracy.</li>
  </ul>
</section>

<section>
  <h2 class="section-title">Projects & Highlights</h2>
  <ul>
    <li>ğŸ©º <strong>Clinical Note Generator</strong> â€“ Fine-tuned LLMs for mental-health documentation (SOAP/DAP formats).</li>
    <li>ğŸ“Š <strong>GenAI Billing Assistant</strong> â€“ Multi-agent RAG system combining SQL and policy text for claim validation.</li>
    <li>âš™ï¸ <strong>SEC Filing Automation</strong> â€“ Graph-based linking of financial tables and narratives with Gemma 2B.</li>
    <li>ğŸ¤ <strong>AIXpert OSS Toolkit</strong> â€“ Co-authored Explainability Survey (2025); built safety-aligned evaluation modules following EU AI Act & NIST RMF.</li>
  </ul>
</section>

<section>
  <h2 class="section-title">Academic & Editorial Service</h2>
  <details>
    <summary>Show Details</summary>
    <div class="mt-2">
      <p>Active reviewer and contributor in AI Safety and Governance research communities.</p>
      <div class="row">
        <div class="col-md-6">
          <strong>Reviewer / Program Committee</strong>
          <ul>
            <li>AAAI 2026 Workshop on AI Governance (AIGOV)</li>
            <li>NeurIPS Workshops â€” LAW, RegML</li>
            <li>Vector Institute Responsible AI projects</li>
          </ul>
        </div>
        <div class="col-md-6">
          <strong>Editorial Roles & Publications</strong>
          <ul>
            <li>Co-author â€” <em>Explainability and Interpretability in Agentic AI (2025)</em></li>
            <li>Contributor â€” AIXpert EU Horizon Project on AI Trust and Accountability</li>
          </ul>
        </div>
      </div>
    </div>
  </details>
</section>

<footer>
  <p>Â© 2025 Sindhuja Chaduvula. All rights reserved.</p>
</footer>

</body>
</html>
